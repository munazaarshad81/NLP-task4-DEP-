Step 1: Import Required Libraries
First, install the required libraries if you haven't already:

bash
Copy code
pip install pandas numpy matplotlib seaborn scikit-learn tensorflow tensorflow-datasets
Now, import the libraries:

python
Copy code
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam




Step 2: Load and Preprocess Data
Load the IMDB Movie Reviews dataset:

python
Copy code
# Load the dataset
(ds_train, ds_test), ds_info = tfds.load(
    'imdb_reviews',
    split=['train', 'test'],
    as_supervised=True,
    with_info=True
)

# Convert the dataset to numpy arrays
train_data, train_labels = [], []
test_data, test_labels = [], []

for text, label in ds_train:
    train_data.append(str(text.numpy()))
    train_labels.append(int(label.numpy()))

for text, label in ds_test:
    test_data.append(str(text.numpy()))
    test_labels.append(int(label.numpy()))

# Convert lists to numpy arrays
train_data = np.array(train_data)
train_labels = np.array(train_labels)
test_data = np.array(test_data)
test_labels = np.array(test_labels)

# Splitting the dataset (optional, since we already have train and test sets)
X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)




Step 3: Text Vectorization and Tokenization
We will use the Tokenizer and pad_sequences from Keras for text vectorization and tokenization:

python
Copy code
# Tokenization and padding
tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_val_seq = tokenizer.texts_to_sequences(X_val)
X_test_seq = tokenizer.texts_to_sequences(test_data)

maxlen = 100
X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen, padding='post', truncating='post')
X_val_pad = pad_sequences(X_val_seq, maxlen=maxlen, padding='post', truncating='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen, padding='post', truncating='post')




Step 4: Building the LSTM Model
We will use an LSTM model for sentiment analysis:

python
Copy code
# Building the LSTM model
model = Sequential([
    Embedding(input_dim=10000, output_dim=128, input_length=maxlen),
    LSTM(128, return_sequences=True),
    Dropout(0.2),
    LSTM(128),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

# Compiling the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Summarize the model
model.summary()




Step 5: Training the Model
Train the model with the training data:

python
Copy code
# Training the model
history = model.fit(X_train_pad, y_train, epochs=5, validation_data=(X_val_pad, y_val), batch_size=64)



Step 6: Evaluating the Model
Evaluate the model's performance on the test data:

python
Copy code
# Evaluating the model
y_pred_dl = (model.predict(X_test_pad) > 0.5).astype("int32")

print(f"Accuracy: {accuracy_score(test_labels, y_pred_dl)}")
print(classification_report(test_labels, y_pred_dl))

# Confusion matrix
conf_mat_dl = confusion_matrix(test_labels, y_pred_dl)
sns.heatmap(conf_mat_dl, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - LSTM')
plt.show()



Step 7: Visualization
(Optional) Visualize the training process:

python
Copy code
# Plotting accuracy and loss
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.legend()
plt.title('Accuracy')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.legend()
plt.title('Loss')
plt.show()